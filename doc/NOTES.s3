
S3:

  minimum size threshold of file (on local disk) BEFORE uploading to S3;
  below this threshold, use single PUT to upload to S3.  Configurable?

  Above 5GB, multipart uploads MUST be used.

  SFTP uploads will have to use multipart uploads, right?  Will mod_aws
  need to maintain a list of "initiated" multipart S3 uploads, such that on
  session exit, they can be aborted, similar to what mod_sftp does?

  Fun with proxies:
    https://github.com/s3fs-fuse/s3fs-fuse/issues/324

  FYI, document that to use mod_aws+S3 via HTTP proxy, set the http_proxy
  environment variable, e.g.:

    SetEnv http_proxy ...

  as libcurl will, per its documentation, honor it.


  Will mod_aws need to be able configure libcurl to work through a forward
  proxy?  (Probably not; wait until someone requests it.)

  Reasons for supporting SSE:
    https://github.com/s3fs-fuse/s3fs-fuse/issues/49

  i.e. admin might configure S3 bucket policy to REQUIRE SSE.

  API for multipart upload:

    aws_s3_object_open(...)

    aws_s3_object_append(...)

    aws_s3_object_close(int flags)
      flags value indicates success (complete multipart) or
      error (abort multipart)

  Uses a struct for tracking state (mostly offset, part number, etc).  Spool
  incoming data to temp file on disk, to buffer up for optimal upload size
  (e.g. 10-50 MB)?

  Retries:
    HTTP 500, 503
    HTTP 400, but ONLY if AWS_S3_ERROR_CODE_REQUEST_TIMEOUT
    presence of Location response header?
    What about AWS_S3_ERROR_CODE_SERVICE_UNAVAILABLE, SLOW_DOWN?

    Exponential backoff, with jittering:
      https://www.awsarchitectureblog.com/2015/03/backoff.html
      http://stackoverflow.com/questions/19657361/how-to-implement-exponential-backoff-in-this-simple-mutex-implementation
      http://docs.aws.amazon.com/general/latest/gr/api-retries.html
      https://news.ycombinator.com/item?id=9206090

   Consider adding/using Content-MD5 request/response header, for additional
   end-to-end integrity/corruption checking.  The TLS part only covers the
   content from mod_aws to S3's frontend servers; who knows what happens to
   that content behind those frontend servers, or during storage.  Thus
   aws_s3_content_md5(), or aws_s3_conn_content_md5(), or even
   aws_s3_object_content_md5()?  Given streamy downloads of S3 objects, we
   should always calculate the md5 (e.g. via MD5Update()).

  Maximum key size?  1024?  This has an impact on the max path length in
  an S3-backed directory!

  Can the request timeout be tweaked, on a curl handle, on a per-request
  basis?  Consider long-lasting PUTs/uploads!  What about any AWS-enforced
  idle timeouts, max request times, etc?  I am thinking of a PUT which
  leads a pause in the curl READFUNCTION callback, while proftpd reads more
  data from the front-end client, and then (somehow) feeds that back to
  the ongoing/open connection (with a resume).

  When mod_mime is available/present, use it to determine a Content-Type
  to set for the uploaded objects?

  Interoperability with s3fs, S3Fox, AWS Console S3 objects:

    AWS Console shows as "folders" any S3 object whose key ends in "/"
    (preferably of size == 0).

    s3fs stores/caches its file info via x-amz-meta-mode

  Trouble with URL-encoding object keys -- what if the key itself contains
  a slash?  This leads to failed requests like this:

    2016-10-01 18:23:15,799 [53394] <aws.http:15>: received response '403 Forbidden' for 'https://s3-us-west-2.amazonaws.com/castaglia/src%2Fproftpd-1.3.6rc2.tar.gz.copy' request

  So for now, we do NOT URL-encode the bucket names or object keys.  Caveat
  emptor.


S3 FSIO:

  use of struct stat, and mapping to/from object metadata, happens in
  this API, *not* in the S3 object API.

  xattr support, and S3 limits on request headers (when setting metadata).
  Consider a pr_table_size() function, which calculates/totals the number
  of bytes of keys and values in a table, for calculating things like this?

  How does s3fs encode/store its file metadata?  Have they encountered any
  gotchas that I'll want to know about?  Should mod_aws_s3 maintain
  interoperability with the other metadata headers that tools like s3fs,
  s3cmd, s3sync, etc maintain?

    In https://github.com/s3fs-fuse/.../s3fs_util.cpp#is_need_check_obj_detail()
    was seen the following:

      x-amz-meta-mode
      x-amz-meta-mtime
      x-amz-meta-uid
      x-amz-meta-gid
      x-amz-meta-owner
      x-amz-meta-group
      x-amz-meta-permissions
      x-amz-meta-xattr
        https://github.com/s3fs-fuse/s3fs-fuse/blob/master/src/common.h#L113

    This requests that s3cmd be compatible with s3fs-fuse in this regard:

      https://github.com/bloomreach/s4cmd/issues/19

    What will mod_aws do for existing x-amz-meta values for an object, e.g.
    when copying?  Maybe always do aws_s3_object_stat() to get metadata
    inside of aws_s3_object_copy(), so that it always Does The Right Thing(tm)?

    What if two objects have the same UID, but different owner names?

  chown(2), chmod(2), chgrp(2), utimes(2) operations on a file require
  changing its metadata.  And changing the metadata for an S3 object MUST
  be done via an object copy, as object metadata is immutable once an object
  has been created.  However, keep in mind THIS caveat, for the
  x-amz-metadata-directive, from the Copy Object docs:

    Default: COPY

    Valid values: COPY | REPLACE

    Constraints: Values other than COPY or REPLACE result in an immediate
    400-based error response. You cannot copy an object to itself unless the
    MetadataDirective header is specified and its value set to REPLACE.

  SO, for the above ch*() FSIO operations, we use copy WITH new object
  metadata, and make sure that the aws_s3_object_copy() function Does The
  Right Thing(tm).

  See:
    http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html


  Caching: Consider a read-many file stored in S3.  Serving that file from
  local disk is definitely faster (and cheaper) than proxying it from S3 to
  the requesting client.  Thus mod_aws SHOULD offer the option of caching
  the file on local disk.  This would entail the normal TTL considers,
  max number of files to cache, etc etc.  In effect, though, this would be
  similar to the "caching" of directory entries that I'm considering.


FTP/SFTP mapping:

  Initially, have PRE_CMD handler for STOR, which blocks any resumed uploads;
  it's not (easily) possible for FTP/SFTP.

  For the directory listing case, WHAT IF mod_aws maintained a
  "directory listing" object for a given bucket prefix, such that ONLY that
  S3 object needed to be downloaded, rather than the brute-force HTTP request
  heavy approach?  Doing so might speed up directory listings, but would also
  require a download/upload of that object, each time an s3 object was
  added/removed.  A potential trick to keep in mind; I suspect that something
  similar may be done by tools like s3fs (which uses its own FUSE cache).
  See:

    http://stackoverflow.com/questions/10989880/efficient-way-to-extract-files-and-meta-data-from-amazon-s3

  And, for performance, this "directory index" object does not have to cover
  ALL of the objects; it too can be partitioned into multiple indexes, based
  on e.g. key ranges.  (Could be a SQLite database index, for example!)  One
  gotcha with this "directory index" approach is that of multiple servers
  uploading files to the same directory/bucket at the same time -- how to
  synchronize access to thta "directory index" object, and prevent clobbering.
  Maybe using the if-not-modified-since/if-none-match approach for an
  object copy?

  ALTERNATIVELY, mod_aws could simply maintain the directory entries on
  local disk.  That way, we get all of the struct stat data (except for size),
  and it makes directory listings MUCH faster/easier.  Doing so would also
  help support symlinks, AND local debugging.  (It might look odd, via the
  shell, seeing a bunch of zero-length files, though.)


    Q: What data consistency model does Amazon S3 employ?

    Amazon S3 buckets in all Regions provide read-after-write consistency
    for PUTS of new objects and eventual consistency for overwrite PUTS and
    DELETES. 

  From:

    https://aws.amazon.com/s3/faqs/

  This means that we ONLY want to show directory listings from the on-disk
  files, NOT from the S3 bucket key listening.


  What if mod_aws installed its own RETR, STOR handles entirely?  That way,
  mod_aws would be in complete control of the IO on the data connection...


Core API:

  Need pr_table_knext(), if you want to get the size of the key that is
  returned.  pr_table_next() does not do this.  (Needed, e.g., for implementing
  pr_table_size() for calculating size of keys/values in table.)


Performance Tuning:

  https://cloudnative.io/blog/2015/01/aws-s3-performance-tuning/
  http://blog.zachbjornson.com/2015/12/29/cloud-storage-performance.html
  https://aws.amazon.com/blogs/aws/amazon-s3-performance-tips-tricks-seattle-hiring-event/

  https://wblinks.com/notes/aws-tips-i-wish-id-known-before-i-started/
    Particularly this tip:

      Use '-' instead of '.' in bucket names for SSL.
        If you ever want to use your bucket over SSL, using a "." will cause
        you to get certificate mismatch errors. You can't change bucket names
        once you've created them, so you'd have to copy everything to a new
        bucket.

    https://wblinks.com/notes/aws-tips-i-wish-id-known-before-i-started/#s3

Future features:

  Support for HTTP compression, specifically requesting compressed responses
  from AWS?  mod_aws may need to the compression itself, i.e. compress
  the data on upload, sent the Content-Encoding: gzip header, and decompress
  on download from S3.  Might save on number of bytes transferred (thus costs).
    http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/ServingCompressedFiles.html#CompressedS3
    https://github.com/s3tools/s3cmd/issues/396
    https://curl.haxx.se/libcurl/c/CURLOPT_ACCEPT_ENCODING.html
    https://curl.haxx.se/libcurl/c/CURLOPT_HTTP_CONTENT_DECODING.html
    https://curl.haxx.se/dev/readme-encoding.html

  Support for different storage classes (per-directory?)

  Support for server-side encryption (per-directory?)
    https://github.com/s3fs-fuse/s3fs-fuse/issues/49

  Support for CloudFront, for read-only/read-heavy directories?

  Performance improvements via better object naming:
    https://cloudnative.io/blog/2015/01/aws-s3-performance-tuning/

  Note that this could introduce backward compatibility hacks.  Consider the
  possibility of uploading each S3 object, getting its Content-MD5, then
  using COPY to rename that object, with the Content-MD5 as the object
  name prefix.  How expensive is an object copy, where just the name is
  changed (and the old object deleted)?

  Support for TTL via object expiration?
    https://launchbylunch.com/posts/2014/Jan/29/aws-tips/#s3

  This is best done on a per-bucket basis, via lifecycle policies:
    https://aws.amazon.com/blogs/aws/amazon-s3-object-expiration/


Related:

  https://objectivefs.com/userguide#memory-cache
  https://github.com/ceph/libs3
  https://github.com/s3fs-fuse/s3fs-fuse 
  http://serverfault.com/questions/554086/move-files-from-ftp-server-to-s3
